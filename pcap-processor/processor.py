# pcap-processor/processor.py
import os
import subprocess
import json
import time
from pathlib import Path
import pandas as pd
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import NoBrokersAvailable

KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")

# This loop makes the script more resilient to startup timing issues.
print("PCAP Processor service started. Attempting to connect to Kafka...", flush=True)
while True:
    try:
        job_consumer = KafkaConsumer(
            'pcap_jobs',
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            auto_offset_reset='earliest',
            group_id='pcap_processor_group',
            api_version=(0, 10, 2)
        )
        print("Successfully connected to Kafka. Waiting for jobs...", flush=True)
        break # Exit loop if connection is successful
    except NoBrokersAvailable:
        print("Kafka not ready, waiting 5 seconds to retry...", flush=True)
        time.sleep(5)

results_producer = KafkaProducer(
    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    api_version=(0, 10, 2)
)

for message in job_consumer:
    filename = message.value.get('filename')
    if not filename:
        continue

    pcap_path = f"/pcap_storage/{filename}"
    print(f"Received job. Analyzing file: {pcap_path}", flush=True)

    try:
        # Run Zeek on the PCAP file in a temporary directory
        subprocess.run(
            ["zeek", "-r", pcap_path,"policy/tuning/json-logs.zeek"],
            cwd="/tmp",
            capture_output=True,
            text=True,
            check=True # This will raise an error if Zeek fails
        )
        print("Zeek analysis complete.", flush=True)
    except subprocess.CalledProcessError as e:
        print(f"Zeek failed to process the file '{filename}': {e.stderr}", flush=True)
        continue # Skip to the next message

    # Check if Zeek produced a sip.log and publish results
    sip_log_path = Path("/tmp/sip.log")
    if sip_log_path.exists():
        print("Found sip.log. Publishing results to 'voip_logs' topic...", flush=True)
        with open(sip_log_path) as f:
            for line in f:
                if line.startswith("#"):
                    continue
                try:
                    log_data = json.loads(line)
                    log_data['fields'] = {'log_type': 'sip'}
                    log_data["source"] = f"pcap:{filename}" # Add source context
                    results_producer.send('voip_logs', log_data)
                except json.JSONDecodeError:
                    print(f"Skipping malformed line in sip.log: {line.strip()}", flush=True)
        results_producer.flush()
        print(f"Finished publishing results for {filename}.", flush=True)
        os.remove(sip_log_path) # Clean up for the next run
    else:
        print(f"No sip.log generated by Zeek for {filename}.", flush=True)

    ssl_log_path = Path("/tmp/ssl.log")
    conn_log_path = Path("/tmp/conn.log")
    weird_log_path = Path("/tmp/weird.log")
    
    if ssl_log_path.exists() and conn_log_path.exists() and weird_log_path.exists():
        print("Found ssl.log and conn.log. Analyzing for encrypted calls...", flush=True)
        try:
            # Read ssl.log and filter for potential SIP-over-TLS on port 5061
            ssl_records = [json.loads(line) for line in open(ssl_log_path) if not line.startswith("#")]
            ssl_records.extend(json.loads(line) for line in open(weird_log_path) if not line.startswith("#"))
            ssl_df = pd.DataFrame(ssl_records)
            ssl_df['ts'] = pd.to_datetime(ssl_df['ts'], unit='s')
            encrypted_sip = ssl_df[
                (ssl_df['id.resp_p'] == 5061) | (ssl_df['id.orig_p'] == 5061)
            ]

            if not encrypted_sip.empty:
                # Read conn.log and filter for UDP (RTP/SRTP) traffic
                conn_records = [json.loads(line) for line in open(conn_log_path) if not line.startswith("#")]
                conn_records.extend(json.loads(line) for line in open(weird_log_path) if not line.startswith("#"))
                conn_df = pd.DataFrame(conn_records)
                conn_df['ts'] = pd.to_datetime(conn_df['ts'], unit='s')
                media_flows = conn_df[conn_df['proto'] == 'udp'].sort_values('ts')

                # Correlate signaling with media flows
                merged = pd.merge_asof(
                    encrypted_sip.sort_values('ts'),
                    media_flows,
                    on='ts',
                    by=['id.orig_h', 'id.resp_h'],
                    direction='forward',
                    tolerance=pd.Timedelta('5s')
                )
                
                inferred_calls = merged.dropna(subset=['uid_y']) # Filter for successful correlations

                print(f"Inferred {len(inferred_calls)} encrypted calls.", flush=True)
                for _, call in inferred_calls.iterrows():
                    # Create a new document that looks like a SIP log for our consumer
                    inferred_log = {
                        "ts": call['ts'].timestamp(),
                        "uid": call['uid_y'], # Use the UID from the connection log
                        "id.orig_h": call['id.orig_h'],
                        "id.orig_p": call['id.orig_p_y'],
                        "id.resp_h": call['id.resp_h'],
                        "id.resp_p": call['id.resp_p_y'],
                        "duration": call['duration'],
                        "call_id": f"inferred-{call['uid_y']}",
                        "tags": ["encrypted_sip", "inferred_call"],
                        "fields": {"log_type": "sip"} # Label it as a SIP log for the consumer
                    }
                    results_producer.send('voip_logs', inferred_log)
            
            os.remove(ssl_log_path)
            os.remove(conn_log_path)
        except Exception as e:
            print(f"Error during encrypted call analysis: {e}", flush=True)

    results_producer.flush()
    print(f"Finished publishing results for {filename}.", flush=True)
